{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import acquire\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_readme_articles(urls, cached=False):\n",
    "    '''\n",
    "    This function takes in a list of GitHub Repo urls and a parameter\n",
    "    with default cached == False which scrapes the title, text, and language for each url, \n",
    "    creates a list of dictionary of features,converts list to df, and returns df.\n",
    "    If cached == True, the function returns a df from a json file.\n",
    "    '''\n",
    "    if cached == True:\n",
    "        df = pd.read_json('project_readme.json')\n",
    "        \n",
    "    # cached == False completes a fresh scrape for df     \n",
    "    else:\n",
    "\n",
    "        # Create an empty list to hold dictionaries\n",
    "        text = []\n",
    "\n",
    "        # Loop through each url in our list of urls\n",
    "        for url in urls:\n",
    "\n",
    "            # Make request and soup object using helper\n",
    "            soup = make_soup(url)\n",
    "\n",
    "            # Save the title of each repo in variable title\n",
    "            title = soup.select('h1', class_=\"Label Label--outline v-align-middle\")[0].text\n",
    "\n",
    "            # Save the text in each repo to variable text\n",
    "            content = soup.select('article', class_=\"markdown-body entry-content container-lg\")[0].text\n",
    "            \n",
    "            # Save the language of each repo in variable language\n",
    "            language = soup.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text\n",
    "\n",
    "            # Create a dictionary holding the title and content for each blog\n",
    "            repo = {'title': title, 'content': content, 'language': language}\n",
    "\n",
    "            # Add each dictionary to the articles list of dictionaries\n",
    "            text.append(repo)\n",
    "            \n",
    "        # convert our list of dictionaries to a df\n",
    "        df = pd.DataFrame(text)\n",
    "\n",
    "        # Write df to a json file for faster access\n",
    "        df.to_json('project_readme.json')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    def basic_clean(text):\n",
    "        text = (unicodedata.normalize('NFKD', text.lower())\n",
    "                .encode('ascii', 'ignore') # ascii to reduce noise\n",
    "                .decode('utf-8', 'ignore') # decode using utf-8\n",
    "               )\n",
    "        return re.sub(r\"[^a-z0-9\\s]\", '', text)\n",
    "\n",
    "    def tokenize(string):\n",
    "        '''\n",
    "        This function takes in a string and\n",
    "        returns a tokenized string.\n",
    "        '''\n",
    "        # Create tokenizer.\n",
    "        tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "        # Use tokenizer\n",
    "        string = tokenizer.tokenize(string, return_str=True)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def lemmatize(string):\n",
    "        '''\n",
    "        This function takes in string for and\n",
    "        returns a string with words lemmatized.\n",
    "        '''\n",
    "        # Create the lemmatizer.\n",
    "        wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "        lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "\n",
    "        # Join our list of words into a string again and assign to a variable.\n",
    "        string = ' '.join(lemmas)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "        '''\n",
    "        This function takes in a string, optional extra_words and exclude_words parameters\n",
    "        with default empty lists and returns a string.\n",
    "        '''\n",
    "        # Create stopword_list.\n",
    "        stopword_list = stopwords.words('english')\n",
    "\n",
    "        # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "        stopword_list = set(stopword_list) - set(exclude_words)\n",
    "        # Add in 'extra_words' to stopword_list.\n",
    "        stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "        # Split words in string.\n",
    "        words = string.split()\n",
    "\n",
    "        # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "        filtered_words = [word for word in words if word not in stopword_list]\n",
    "\n",
    "        # Join words in the list back into strings and assign to a variable.\n",
    "        string_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "        return string_without_stopwords\n",
    "    \n",
    "    df['title'] = df.title.apply(basic_clean)\n",
    "    df['title'] = df.title.apply(tokenize)\n",
    "    df['title'] = df.title.apply(lemmatize)\n",
    "    df['language'] = df.language.apply(basic_clean)\n",
    "    df['language'] = df.language.apply(tokenize)\n",
    "    df['language'] = df.language.apply(lemmatize)\n",
    "    #\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    df['language'] = df['language'].str.translate(remove_digits)\n",
    "    #\n",
    "    df['text_cleaned'] = df.content.apply(basic_clean)\n",
    "    df['text_tokenized'] = df.text_cleaned.apply(tokenize)\n",
    "    df['text_lemmatized'] = df.text_tokenized.apply(lemmatize)\n",
    "    df['text_filtered'] = df.text_lemmatized.apply(remove_stopwords)\n",
    "    # Add column with list of words\n",
    "    words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.text_filtered]\n",
    "    df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)\n",
    "    # Adds colum with lenght of word list\n",
    "    df['doc_length'] = [len(wordlist) for wordlist in df.words]\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nfreeCodeCamp\\n\\n/\\n\\nfreeCodeCamp\\n\\n</td>\n",
       "      <td>\\n\\n\\n\\n\\nfreeCodeCamp.org's open-source codeb...</td>\n",
       "      <td>\\n\\nJavaScript\\n91.3%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n996icu\\n\\n/\\n\\n996.ICU\\n\\n</td>\n",
       "      <td>996.ICU\\nPlease note that there exists NO othe...</td>\n",
       "      <td>\\n\\nRust\\n59.9%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\nvuejs\\n\\n/\\n\\nvue\\n\\n</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupporting Vue.js\\nVue.j...</td>\n",
       "      <td>\\n\\nJavaScript\\n97.7%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nCSolitaire\\n\\n/\\n\\nnatural-language-proc...</td>\n",
       "      <td>Natural Language Processing\\n\\n\\n\\nSummary\\nNL...</td>\n",
       "      <td>\\n\\nJupyter Notebook\\n99.9%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\nfacebook\\n\\n/\\n\\nreact\\n\\n</td>\n",
       "      <td>React Â·    \\nReact is a JavaScript library for...</td>\n",
       "      <td>\\n\\nJavaScript\\n95.0%\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        \\n\\n\\nfreeCodeCamp\\n\\n/\\n\\nfreeCodeCamp\\n\\n   \n",
       "1                   \\n\\n\\n996icu\\n\\n/\\n\\n996.ICU\\n\\n   \n",
       "2                        \\n\\n\\nvuejs\\n\\n/\\n\\nvue\\n\\n   \n",
       "3  \\n\\n\\nCSolitaire\\n\\n/\\n\\nnatural-language-proc...   \n",
       "4                   \\n\\n\\nfacebook\\n\\n/\\n\\nreact\\n\\n   \n",
       "\n",
       "                                             content  \\\n",
       "0  \\n\\n\\n\\n\\nfreeCodeCamp.org's open-source codeb...   \n",
       "1  996.ICU\\nPlease note that there exists NO othe...   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupporting Vue.js\\nVue.j...   \n",
       "3  Natural Language Processing\\n\\n\\n\\nSummary\\nNL...   \n",
       "4  React Â·    \\nReact is a JavaScript library for...   \n",
       "\n",
       "                        language  \n",
       "0        \\n\\nJavaScript\\n91.3%\\n  \n",
       "1              \\n\\nRust\\n59.9%\\n  \n",
       "2        \\n\\nJavaScript\\n97.7%\\n  \n",
       "3  \\n\\nJupyter Notebook\\n99.9%\\n  \n",
       "4        \\n\\nJavaScript\\n95.0%\\n  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here cached == False, so the function will do a fresh scrape of the urls and write data to a json file.\n",
    "\n",
    "urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "        'https://github.com/996icu/996.ICU',\n",
    "        'https://github.com/vuejs/vue',\n",
    "        'https://github.com/CSolitaire/natural-language-processing-exercises',\n",
    "        'https://github.com/facebook/react']\n",
    "\n",
    "df = get_readme_articles(urls=urls, cached=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_filtered</th>\n",
       "      <th>words</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>freecodecamp freecodecamp</td>\n",
       "      <td>\\n\\n\\n\\n\\nfreeCodeCamp.org's open-source codeb...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>\\n\\n\\n\\n\\nfreecodecamporgs opensource codebase...</td>\n",
       "      <td>freecodecamporgs opensource codebase and curri...</td>\n",
       "      <td>freecodecamporgs opensource codebase and curri...</td>\n",
       "      <td>freecodecamporgs opensource codebase curriculu...</td>\n",
       "      <td>[freecodecamporgs, opensource, codebase, curri...</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>996icu 996icu</td>\n",
       "      <td>996.ICU\\nPlease note that there exists NO othe...</td>\n",
       "      <td>rust</td>\n",
       "      <td>996icu\\nplease note that there exists no other...</td>\n",
       "      <td>996icu\\nplease note that there exists no other...</td>\n",
       "      <td>996icu please note that there exists no other ...</td>\n",
       "      <td>996icu please note exists official account app...</td>\n",
       "      <td>[996icu, please, note, exists, official, accou...</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vuejs vue</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupporting Vue.js\\nVue.j...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsupporting vuejs\\nvuejs ...</td>\n",
       "      <td>supporting vuejs\\nvuejs is an mitlicensed open...</td>\n",
       "      <td>supporting vuejs vuejs is an mitlicensed open ...</td>\n",
       "      <td>supporting vuejs vuejs mitlicensed open source...</td>\n",
       "      <td>[supporting, vuejs, vuejs, mitlicensed, open, ...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csolitaire naturallanguageprocessingexercises</td>\n",
       "      <td>Natural Language Processing\\n\\n\\n\\nSummary\\nNL...</td>\n",
       "      <td>jupyter notebook</td>\n",
       "      <td>natural language processing\\n\\n\\n\\nsummary\\nnl...</td>\n",
       "      <td>natural language processing\\n\\n\\n\\nsummary\\nnl...</td>\n",
       "      <td>natural language processing summary nlp or nat...</td>\n",
       "      <td>natural language processing summary nlp natura...</td>\n",
       "      <td>[natural, language, processing, summary, nlp, ...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook react</td>\n",
       "      <td>React Â·    \\nReact is a JavaScript library for...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>react     \\nreact is a javascript library for ...</td>\n",
       "      <td>react \\nreact is a javascript library for buil...</td>\n",
       "      <td>react react is a javascript library for buildi...</td>\n",
       "      <td>react react javascript library building user i...</td>\n",
       "      <td>[react, react, javascript, library, building, ...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title  \\\n",
       "0                      freecodecamp freecodecamp   \n",
       "1                                  996icu 996icu   \n",
       "2                                      vuejs vue   \n",
       "3  csolitaire naturallanguageprocessingexercises   \n",
       "4                                 facebook react   \n",
       "\n",
       "                                             content           language  \\\n",
       "0  \\n\\n\\n\\n\\nfreeCodeCamp.org's open-source codeb...        javascript    \n",
       "1  996.ICU\\nPlease note that there exists NO othe...              rust    \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupporting Vue.js\\nVue.j...        javascript    \n",
       "3  Natural Language Processing\\n\\n\\n\\nSummary\\nNL...  jupyter notebook    \n",
       "4  React Â·    \\nReact is a JavaScript library for...        javascript    \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  \\n\\n\\n\\n\\nfreecodecamporgs opensource codebase...   \n",
       "1  996icu\\nplease note that there exists no other...   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsupporting vuejs\\nvuejs ...   \n",
       "3  natural language processing\\n\\n\\n\\nsummary\\nnl...   \n",
       "4  react     \\nreact is a javascript library for ...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  freecodecamporgs opensource codebase and curri...   \n",
       "1  996icu\\nplease note that there exists no other...   \n",
       "2  supporting vuejs\\nvuejs is an mitlicensed open...   \n",
       "3  natural language processing\\n\\n\\n\\nsummary\\nnl...   \n",
       "4  react \\nreact is a javascript library for buil...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  freecodecamporgs opensource codebase and curri...   \n",
       "1  996icu please note that there exists no other ...   \n",
       "2  supporting vuejs vuejs is an mitlicensed open ...   \n",
       "3  natural language processing summary nlp or nat...   \n",
       "4  react react is a javascript library for buildi...   \n",
       "\n",
       "                                       text_filtered  \\\n",
       "0  freecodecamporgs opensource codebase curriculu...   \n",
       "1  996icu please note exists official account app...   \n",
       "2  supporting vuejs vuejs mitlicensed open source...   \n",
       "3  natural language processing summary nlp natura...   \n",
       "4  react react javascript library building user i...   \n",
       "\n",
       "                                               words  doc_length  \n",
       "0  [freecodecamporgs, opensource, codebase, curri...         707  \n",
       "1  [996icu, please, note, exists, official, accou...         456  \n",
       "2  [supporting, vuejs, vuejs, mitlicensed, open, ...         256  \n",
       "3  [natural, language, processing, summary, nlp, ...         207  \n",
       "4  [react, react, javascript, library, building, ...         314  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Everything is good to go except the 'language' call**\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "#         'https://github.com/996icu/996.ICU',\n",
    "#         'https://github.com/vuejs/vue',\n",
    "#         'https://github.com/EbookFoundation/free-programming-books',\n",
    "#         'https://github.com/facebook/react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soup Object\n",
    "soup = make_soup('https://github.com/freeCodeCamp/freeCodeCamp')\n",
    "soup1 = make_soup('https://github.com/996icu/996.ICU')\n",
    "soup2 = make_soup('https://github.com/vuejs/vue')\n",
    "soup3 = make_soup('https://github.com/EbookFoundation/free-programming-books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nJavaScript\\n91.3%\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Language\n",
    "soup.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRust\\n59.9%\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup1.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nJavaScript\\n97.7%\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f166d8dd747b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li.d-inline:nth-child(1) > a:nth-child(1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "soup3.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_digit(s):\n",
    "    no_digits = []\n",
    "    # Iterate through the string, adding non-numbers to the no_digits list\n",
    "    for i in s:\n",
    "        if not i.isdigit():\n",
    "            no_digits.append(i)\n",
    "        return no_digits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
