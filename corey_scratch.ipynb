{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import acquire\n",
    "import prepare\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_soup(url):\n",
    "#     '''\n",
    "#     This helper function takes in a url and requests and parses HTML\n",
    "#     returning a soup object.\n",
    "#     '''\n",
    "#     response = get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     return soup\n",
    "\n",
    "# def get_readme_articles(urls, cached=False):\n",
    "#     '''\n",
    "#     This function takes in a list of GitHub Repo urls and a parameter\n",
    "#     with default cached == False which scrapes the title, text, and language for each url, \n",
    "#     creates a list of dictionary of features,converts list to df, and returns df.\n",
    "#     If cached == True, the function returns a df from a json file.\n",
    "#     '''\n",
    "#     if cached == True:\n",
    "#         df = pd.read_json('project_readme.json')\n",
    "        \n",
    "#     # cached == False completes a fresh scrape for df     \n",
    "#     else:\n",
    "\n",
    "#         # Create an empty list to hold dictionaries\n",
    "#         text = []\n",
    "\n",
    "#         # Loop through each url in our list of urls\n",
    "#         for url in urls:\n",
    "\n",
    "#             # Make request and soup object using helper\n",
    "#             soup = make_soup(url)\n",
    "\n",
    "#             # Save the title of each repo in variable title\n",
    "#             title = soup.select('h1', class_=\"Label Label--outline v-align-middle\")[0].text\n",
    "\n",
    "#             # Save the text in each repo to variable text\n",
    "#             content = soup.select('article', class_=\"markdown-body entry-content container-lg\")[0].text\n",
    "            \n",
    "#             # Save the language of each repo in variable language\n",
    "#             language = soup.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text\n",
    "\n",
    "#             # Create a dictionary holding the title and content for each blog\n",
    "#             repo = {'title': title, 'content': content, 'language': language}\n",
    "\n",
    "#             # Add each dictionary to the articles list of dictionaries\n",
    "#             text.append(repo)\n",
    "            \n",
    "#         # convert our list of dictionaries to a df\n",
    "#         df = pd.DataFrame(text)\n",
    "\n",
    "#         # Write df to a json file for faster access\n",
    "#         df.to_json('project_readme.json')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     def basic_clean(text):\n",
    "#         text = (unicodedata.normalize('NFKD', text.lower())\n",
    "#                 .encode('ascii', 'ignore') # ascii to reduce noise\n",
    "#                 .decode('utf-8', 'ignore') # decode using utf-8\n",
    "#                )\n",
    "#         return re.sub(r\"[^a-z0-9\\s]\", '', text)\n",
    "\n",
    "#     def tokenize(string):\n",
    "#         '''\n",
    "#         This function takes in a string and\n",
    "#         returns a tokenized string.\n",
    "#         '''\n",
    "#         # Create tokenizer.\n",
    "#         tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "#         # Use tokenizer\n",
    "#         string = tokenizer.tokenize(string, return_str=True)\n",
    "\n",
    "#         return string\n",
    "\n",
    "#     def lemmatize(string):\n",
    "#         '''\n",
    "#         This function takes in string for and\n",
    "#         returns a string with words lemmatized.\n",
    "#         '''\n",
    "#         # Create the lemmatizer.\n",
    "#         wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#         # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "#         lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "\n",
    "#         # Join our list of words into a string again and assign to a variable.\n",
    "#         string = ' '.join(lemmas)\n",
    "\n",
    "#         return string\n",
    "\n",
    "#     def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "#         '''\n",
    "#         This function takes in a string, optional extra_words and exclude_words parameters\n",
    "#         with default empty lists and returns a string.\n",
    "#         '''\n",
    "#         # Create stopword_list.\n",
    "#         stopword_list = stopwords.words('english')\n",
    "\n",
    "#         # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "#         stopword_list = set(stopword_list) - set(exclude_words)\n",
    "#         # Add in 'extra_words' to stopword_list.\n",
    "#         stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "#         # Split words in string.\n",
    "#         words = string.split()\n",
    "\n",
    "#         # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "#         filtered_words = [word for word in words if word not in stopword_list]\n",
    "\n",
    "#         # Join words in the list back into strings and assign to a variable.\n",
    "#         string_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "#         return string_without_stopwords\n",
    "    \n",
    "#     df['title'] = df.title.apply(basic_clean)\n",
    "#     df['title'] = df.title.apply(tokenize)\n",
    "#     df['title'] = df.title.apply(lemmatize)\n",
    "#     df['language'] = df.language.apply(basic_clean)\n",
    "#     df['language'] = df.language.apply(tokenize)\n",
    "#     df['language'] = df.language.apply(lemmatize)\n",
    "#     #\n",
    "#     remove_digits = str.maketrans('', '', digits)\n",
    "#     df['language'] = df['language'].str.translate(remove_digits)\n",
    "#     #\n",
    "#     df['text_cleaned'] = df.content.apply(basic_clean)\n",
    "#     df['text_tokenized'] = df.text_cleaned.apply(tokenize)\n",
    "#     df['text_lemmatized'] = df.text_tokenized.apply(lemmatize)\n",
    "#     df['text_filtered'] = df.text_lemmatized.apply(remove_stopwords)\n",
    "#     # Add column with list of words\n",
    "#     words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.text_filtered]\n",
    "#     df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)\n",
    "#     # Adds colum with lenght of word list\n",
    "#     df['doc_length'] = [len(wordlist) for wordlist in df.words]\n",
    "#     return df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the URL's\n",
    "\n",
    "- Focused on URL's that were identified as JavaScript or Python\n",
    "- Opened up searh URL on GitHub, added URLs that fit discription untill 105 were sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "        'https://github.com/Famous/famous',\n",
    "        'https://github.com/vuejs/vue',\n",
    "        'https://github.com/kevana/ui-for-docker,\n",
    "        'https://github.com/facebook/react'\n",
    "        'https://github.com/ipython/ipython'\n",
    "        'https://github.com/microsoft/TypeScript-Handbook'\n",
    "        'https://github.com/airbnb/knowledge-repo'\n",
    "        'https://github.com/rkern/line_profiler'\n",
    "        'https://github.com/babel/babel-preset-env'\n",
    "        'https://github.com/s3tools/s3cmd'\n",
    "        'https://github.com/zzzeek/sqlalchemy'\n",
    "        'https://github.com/thunil/TecoGAN'\n",
    "        'https://github.com/l1ving/youtube-dl'\n",
    "        'https://github.com/mdo/github-buttons'\n",
    "        'https://github.com/StephenGrider/ReactNativeReduxCasts'\n",
    "        'https://github.com/apollographql/apollo'\n",
    "        'https://github.com/wandb/client'\n",
    "        'https://github.com/RasaHQ/rasa_core'\n",
    "        'https://github.com/angular-ui/angular-ui-OLDREPO'\n",
    "        'https://github.com/urwid/urwid'\n",
    "        'https://github.com/timqian/star-history'\n",
    "        'https://github.com/PatrickJS/NG6-starter'\n",
    "        'https://github.com/knrt10/kubernetes-basicLearning'\n",
    "        'https://github.com/nature-of-code/noc-book'\n",
    "        'https://github.com/erikbern/git-of-theseus'\n",
    "        'https://github.com/dortania/OpenCore-Install-Guide'\n",
    "        'https://github.com/Kapeli/Dash-User-Contributions'\n",
    "        'https://github.com/github/docs'\n",
    "        'https://github.com/StephenGrider/redux-code'\n",
    "        'https://github.com/fossasia/meilix'\n",
    "        'https://github.com/StephenGrider/EthereumCasts'\n",
    "        'https://github.com/newren/git-filter-repo'\n",
    "        'https://github.com/mateodelnorte/meta'\n",
    "        'https://github.com/arsaboo/homeassistant-config'\n",
    "        'https://github.com/IronLanguages/main'\n",
    "        'https://github.com/StephenGrider/FullstackReactCode'\n",
    "        'https://github.com/openworm/OpenWorm'\n",
    "        'https://github.com/apache/nano'\n",
    "        'https://github.com/jupyterhub/repo2docker'\n",
    "        'https://github.com/abidrahmank/OpenCV2-Python-Tutorials'\n",
    "        'https://github.com/Ceruleanacg/Personae'\n",
    "        'https://github.com/mtdvio/ru-tech-chats'\n",
    "        'https://github.com/xinntao/EDVR'\n",
    "        'https://github.com/RubensZimbres/Repo-2017'\n",
    "        'https://github.com/ipfs-inactive/js-ipfs-http-client'\n",
    "        'https://github.com/browserpass/browserpass-legacy'\n",
    "        'https://github.com/boston-dynamics/spot-sdk'\n",
    "        'https://github.com/sourcerer-io/hall-of-fame'\n",
    "        'https://github.com/blackorbird/APT_REPORT'\n",
    "        'https://github.com/creationix/howtonode.org'\n",
    "        'https://github.com/jennschiffer/make8bitart'\n",
    "        'https://github.com/wdas/reposado'\n",
    "        'https://github.com/guyzmo/git-repo'\n",
    "        'https://github.com/Netflix/repokid'\n",
    "        'https://github.com/nosarthur/gita'\n",
    "        'https://github.com/harshjv/github-repo-size'\n",
    "        'https://github.com/babel/babel-standalone'\n",
    "        'https://github.com/kevin28520/My-TensorFlow-tutorials'\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        ''\n",
    "        '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "urls = create_url_list()\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4a1ec40bb68a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         'https://github.com/facebook/react']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_readme_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/nlp_project/acquire.py\u001b[0m in \u001b[0;36mget_readme_articles\u001b[0;34m(urls, cached)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Save the language of each repo in variable language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li.d-inline:nth-child(1) > a:nth-child(1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Create a dictionary holding the title and content for each blog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Here cached == False, so the function will do a fresh scrape of the urls and write data to a json file.\n",
    "\n",
    "# urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "#         'https://github.com/996icu/996.ICU',\n",
    "#         'https://github.com/vuejs/vue',\n",
    "#         'https://github.com/CSolitaire/natural-language-processing-exercises',\n",
    "#         'https://github.com/facebook/react']\n",
    "\n",
    "df = acquire.get_readme_articles(urls=urls, cached=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Everything is good to go except the 'language' call**\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "#         'https://github.com/996icu/996.ICU',\n",
    "#         'https://github.com/vuejs/vue',\n",
    "#         'https://github.com/EbookFoundation/free-programming-books',\n",
    "#         'https://github.com/facebook/react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soup Object\n",
    "soup = make_soup('https://github.com/freeCodeCamp/freeCodeCamp')\n",
    "soup1 = make_soup('https://github.com/996icu/996.ICU')\n",
    "soup2 = make_soup('https://github.com/vuejs/vue')\n",
    "soup3 = make_soup('https://github.com/EbookFoundation/free-programming-books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language\n",
    "soup.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_digit(s):\n",
    "    no_digits = []\n",
    "    # Iterate through the string, adding non-numbers to the no_digits list\n",
    "    for i in s:\n",
    "        if not i.isdigit():\n",
    "            no_digits.append(i)\n",
    "        return no_digits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
