{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import acquire\n",
    "import prepare\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_soup(url):\n",
    "#     '''\n",
    "#     This helper function takes in a url and requests and parses HTML\n",
    "#     returning a soup object.\n",
    "#     '''\n",
    "#     response = get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     return soup\n",
    "\n",
    "# def get_readme_articles(urls, cached=False):\n",
    "#     '''\n",
    "#     This function takes in a list of GitHub Repo urls and a parameter\n",
    "#     with default cached == False which scrapes the title, text, and language for each url, \n",
    "#     creates a list of dictionary of features,converts list to df, and returns df.\n",
    "#     If cached == True, the function returns a df from a json file.\n",
    "#     '''\n",
    "#     if cached == True:\n",
    "#         df = pd.read_json('project_readme.json')\n",
    "        \n",
    "#     # cached == False completes a fresh scrape for df     \n",
    "#     else:\n",
    "\n",
    "#         # Create an empty list to hold dictionaries\n",
    "#         text = []\n",
    "\n",
    "#         # Loop through each url in our list of urls\n",
    "#         for url in urls:\n",
    "\n",
    "#             # Make request and soup object using helper\n",
    "#             soup = make_soup(url)\n",
    "\n",
    "#             # Save the title of each repo in variable title\n",
    "#             title = soup.select('h1', class_=\"Label Label--outline v-align-middle\")[0].text\n",
    "\n",
    "#             # Save the text in each repo to variable text\n",
    "#             content = soup.select('article', class_=\"markdown-body entry-content container-lg\")[0].text\n",
    "            \n",
    "#             # Save the language of each repo in variable language\n",
    "#             language = soup.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text\n",
    "\n",
    "#             # Create a dictionary holding the title and content for each blog\n",
    "#             repo = {'title': title, 'content': content, 'language': language}\n",
    "\n",
    "#             # Add each dictionary to the articles list of dictionaries\n",
    "#             text.append(repo)\n",
    "            \n",
    "#         # convert our list of dictionaries to a df\n",
    "#         df = pd.DataFrame(text)\n",
    "\n",
    "#         # Write df to a json file for faster access\n",
    "#         df.to_json('project_readme.json')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Functions\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = (unicodedata.normalize('NFKD', text.lower())\n",
    "            .encode('ascii', 'ignore') # ascii to reduce noise\n",
    "            .decode('utf-8', 'ignore') # decode using utf-8\n",
    "           )\n",
    "    return re.sub(r\"[^a-z0-9\\s]\", '', text)\n",
    "\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the URL's\n",
    "\n",
    "- Focused on URL's that were identified as JavaScript or Python\n",
    "- Opened up searh URL on GitHub, added URLs that fit discription untill 105 were sampled\n",
    "- Ended on search page 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "        'https://github.com/Famous/famous',\n",
    "        'https://github.com/vuejs/vue',\n",
    "        'https://github.com/kevana/ui-for-docker',\n",
    "        'https://github.com/facebook/react',\n",
    "        'https://github.com/ipython/ipython',\n",
    "        'https://github.com/microsoft/TypeScript-Handbook',\n",
    "        'https://github.com/airbnb/knowledge-repo',\n",
    "        'https://github.com/rkern/line_profiler',\n",
    "        'https://github.com/babel/babel-preset-env',\n",
    "        'https://github.com/s3tools/s3cmd',\n",
    "        'https://github.com/zzzeek/sqlalchemy',\n",
    "        'https://github.com/thunil/TecoGAN',\n",
    "        'https://github.com/l1ving/youtube-dl',\n",
    "        'https://github.com/mdo/github-buttons',\n",
    "        'https://github.com/StephenGrider/ReactNativeReduxCasts',\n",
    "        'https://github.com/apollographql/apollo',\n",
    "        'https://github.com/wandb/client',\n",
    "        'https://github.com/RasaHQ/rasa_core',\n",
    "        'https://github.com/angular-ui/angular-ui-OLDREPO',\n",
    "        'https://github.com/urwid/urwid',\n",
    "        'https://github.com/timqian/star-history',\n",
    "        'https://github.com/PatrickJS/NG6-starter',\n",
    "        'https://github.com/knrt10/kubernetes-basicLearning',\n",
    "        'https://github.com/nature-of-code/noc-book',\n",
    "        'https://github.com/erikbern/git-of-theseus',\n",
    "        'https://github.com/dortania/OpenCore-Install-Guide',\n",
    "        'https://github.com/Kapeli/Dash-User-Contributions',\n",
    "        'https://github.com/github/docs',\n",
    "        'https://github.com/StephenGrider/redux-code',\n",
    "        'https://github.com/fossasia/meilix',\n",
    "        'https://github.com/StephenGrider/EthereumCasts',\n",
    "        'https://github.com/newren/git-filter-repo',\n",
    "        'https://github.com/mateodelnorte/meta',\n",
    "        'https://github.com/arsaboo/homeassistant-config',\n",
    "        'https://github.com/IronLanguages/main',\n",
    "        'https://github.com/StephenGrider/FullstackReactCode',\n",
    "        'https://github.com/openworm/OpenWorm',\n",
    "        'https://github.com/apache/nano',\n",
    "        'https://github.com/jupyterhub/repo2docker',\n",
    "        'https://github.com/abidrahmank/OpenCV2-Python-Tutorials',\n",
    "        'https://github.com/Ceruleanacg/Personae',\n",
    "        'https://github.com/mtdvio/ru-tech-chats',\n",
    "        'https://github.com/xinntao/EDVR',\n",
    "        'https://github.com/RubensZimbres/Repo-2017',\n",
    "        'https://github.com/ipfs-inactive/js-ipfs-http-client',\n",
    "        'https://github.com/browserpass/browserpass-legacy',\n",
    "        'https://github.com/boston-dynamics/spot-sdk',\n",
    "        'https://github.com/sourcerer-io/hall-of-fame',\n",
    "        'https://github.com/blackorbird/APT_REPORT',\n",
    "        'https://github.com/creationix/howtonode.org',\n",
    "        'https://github.com/jennschiffer/make8bitart',\n",
    "        'https://github.com/wdas/reposado',\n",
    "        'https://github.com/guyzmo/git-repo',\n",
    "        'https://github.com/Netflix/repokid',\n",
    "        'https://github.com/nosarthur/gita',\n",
    "        'https://github.com/harshjv/github-repo-size',\n",
    "        'https://github.com/babel/babel-standalone',\n",
    "        'https://github.com/kevin28520/My-TensorFlow-tutorials',\n",
    "        'https://github.com/diyhue/diyHue',\n",
    "        'https://github.com/StephenGrider/rn-casts',\n",
    "        'https://github.com/headsetapp/headset-electron',\n",
    "        'https://github.com/StijnMiroslav/top-starred-devs-and-repos-to-follow',\n",
    "        'https://github.com/techgaun/active-forks',\n",
    "        'https://github.com/donnemartin/viz',\n",
    "        'https://github.com/tailwindlabs/tailwindui-vue',\n",
    "        'https://github.com/GitGuardian/gg-shield',\n",
    "        'https://github.com/dtschust/redux-bug-reporter',\n",
    "        'https://github.com/burke-software/django-report-builder',\n",
    "        'https://github.com/antsmartian/lets-build-express',\n",
    "        'https://github.com/MicrosoftDocs/visualstudio-docs',\n",
    "        'https://github.com/earwig/git-repo-updater',\n",
    "        'https://github.com/OpenSourceTogether/Hacktoberfest-2020',\n",
    "        'https://github.com/lightaime/deep_gcns_torch',\n",
    "        'https://github.com/A3M4/YouTube-Report',\n",
    "        'https://github.com/heroku/heroku-repo',\n",
    "        'https://github.com/lambdaji/tf_repos',\n",
    "        'https://github.com/StephenGrider/AdvancedReactNative',\n",
    "        'https://github.com/lightaime/deep_gcns',\n",
    "        'https://github.com/StephenGrider/DockerCasts',\n",
    "        'https://github.com/mappum/gitbanner',\n",
    "        'https://github.com/declare-lab/conv-emotion',\n",
    "        'https://github.com/MarkWuNLP/MultiTurnResponseSelection',\n",
    "        'https://github.com/chriswhong/nyctaxi',\n",
    "        'https://github.com/18F/analytics-reporter',\n",
    "        'https://github.com/npmhub/npmhub',\n",
    "        'https://github.com/kesiev/akihabara',\n",
    "        'https://github.com/ionic-team/ionic-site',\n",
    "        'https://github.com/ros/rosdistro',\n",
    "        'https://github.com/GoogleChromeLabs/tooling.report',\n",
    "        'https://github.com/rpl/flow-coverage-report',\n",
    "        'https://github.com/storybook-eol/react-native-storybook',\n",
    "        'https://github.com/StephenGrider/MongoCasts',\n",
    "        'https://github.com/kundajelab/deeplift',\n",
    "        'https://github.com/microsoft/HealthClinic.biz',\n",
    "        'https://github.com/Redocly/create-openapi-repo',\n",
    "        'https://github.com/philbooth/complexity-report',\n",
    "        'https://github.com/bensmithett/webpack-css-example',\n",
    "        'https://github.com/PhantomInsights/mexican-government-report',\n",
    "        'https://github.com/googlefonts/Inconsolata',\n",
    "        'https://github.com/laincloud/lain',\n",
    "        'https://github.com/auth0/repo-supervisor',\n",
    "        'https://github.com/weightagnostic/weightagnostic.github.io',\n",
    "        'https://github.com/jdorn/php-reports',\n",
    "        'https://github.com/joeldenning/coexisting-vue-microfrontends']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "print(len(urls)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nfreeCodeCamp\\n\\n/\\n\\nfreeCodeCamp\\n\\n</td>\n",
       "      <td>\\n\\n\\n\\n\\nfreeCodeCamp.org's open-source codeb...</td>\n",
       "      <td>\\n\\nJavaScript\\n91.3%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nFamous\\n\\n/\\n\\nfamous\\n\\n</td>\n",
       "      <td>#THIS REPO IS DEPRECATED\\n##Mixed Mode can be ...</td>\n",
       "      <td>\\n\\nJavaScript\\n86.6%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\nvuejs\\n\\n/\\n\\nvue\\n\\n</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupporting Vue.js\\nVue.j...</td>\n",
       "      <td>\\n\\nJavaScript\\n97.7%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nkevana\\n\\n/\\n\\nui-for-docker\\n\\n</td>\n",
       "      <td>UI For Docker\\n\\nThis repo is deprecated. Deve...</td>\n",
       "      <td>\\n\\nJavaScript\\n61.5%\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\nfacebook\\n\\n/\\n\\nreact\\n\\n</td>\n",
       "      <td>React ·    \\nReact is a JavaScript library for...</td>\n",
       "      <td>\\n\\nJavaScript\\n95.0%\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0  \\n\\n\\nfreeCodeCamp\\n\\n/\\n\\nfreeCodeCamp\\n\\n   \n",
       "1              \\n\\n\\nFamous\\n\\n/\\n\\nfamous\\n\\n   \n",
       "2                  \\n\\n\\nvuejs\\n\\n/\\n\\nvue\\n\\n   \n",
       "3       \\n\\n\\nkevana\\n\\n/\\n\\nui-for-docker\\n\\n   \n",
       "4             \\n\\n\\nfacebook\\n\\n/\\n\\nreact\\n\\n   \n",
       "\n",
       "                                             content                 language  \n",
       "0  \\n\\n\\n\\n\\nfreeCodeCamp.org's open-source codeb...  \\n\\nJavaScript\\n91.3%\\n  \n",
       "1  #THIS REPO IS DEPRECATED\\n##Mixed Mode can be ...  \\n\\nJavaScript\\n86.6%\\n  \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSupporting Vue.js\\nVue.j...  \\n\\nJavaScript\\n97.7%\\n  \n",
       "3  UI For Docker\\n\\nThis repo is deprecated. Deve...  \\n\\nJavaScript\\n61.5%\\n  \n",
       "4  React ·    \\nReact is a JavaScript library for...  \\n\\nJavaScript\\n95.0%\\n  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here cached == False, so the function will do a fresh scrape of the urls and write data to a json file.\n",
    "\n",
    "# urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "#         'https://github.com/996icu/996.ICU',\n",
    "#         'https://github.com/vuejs/vue',\n",
    "#         'https://github.com/CSolitaire/natural-language-processing-exercises',\n",
    "#         'https://github.com/facebook/react']\n",
    "\n",
    "df = acquire.get_readme_articles(urls=urls, cached=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 105 entries, 0 to 104\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     105 non-null    object\n",
      " 1   content   105 non-null    object\n",
      " 2   language  105 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df.title.apply(basic_clean)\n",
    "df['title'] = df.title.apply(tokenize)\n",
    "df['title'] = df.title.apply(lemmatize)\n",
    "df['language'] = df.language.apply(basic_clean)\n",
    "df['language'] = df.language.apply(tokenize)\n",
    "df['language'] = df.language.apply(lemmatize)\n",
    "#\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "df['language'] = df['language'].str.translate(remove_digits)\n",
    "#\n",
    "df['text_cleaned'] = df.content.apply(basic_clean)\n",
    "df['text_tokenized'] = df.text_cleaned.apply(tokenize)\n",
    "df['text_lemmatized'] = df.text_tokenized.apply(lemmatize)\n",
    "df['text_filtered'] = df.text_lemmatized.apply(remove_stopwords)\n",
    "# Add column with list of words\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.text_filtered]\n",
    "df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)\n",
    "# Adds colum with lenght of word list\n",
    "df['doc_length'] = [len(wordlist) for wordlist in df.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 105 entries, 0 to 104\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            105 non-null    object\n",
      " 1   content          105 non-null    object\n",
      " 2   language         105 non-null    object\n",
      " 3   text_cleaned     105 non-null    object\n",
      " 4   text_tokenized   105 non-null    object\n",
      " 5   text_lemmatized  105 non-null    object\n",
      " 6   text_filtered    105 non-null    object\n",
      " 7   words            105 non-null    object\n",
      " 8   doc_length       105 non-null    int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = clean_data(df)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Everything is good to go except the 'language' call**\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = ['https://github.com/freeCodeCamp/freeCodeCamp',\n",
    "#         'https://github.com/996icu/996.ICU',\n",
    "#         'https://github.com/vuejs/vue',\n",
    "#         'https://github.com/EbookFoundation/free-programming-books',\n",
    "#         'https://github.com/facebook/react']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_soup(url):\n",
    "#     '''\n",
    "#     This helper function takes in a url and requests and parses HTML\n",
    "#     returning a soup object.\n",
    "#     '''\n",
    "#     response = get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Soup Object\n",
    "# soup = make_soup('https://github.com/facebook/react')\n",
    "# soup1 = make_soup('https://github.com/996icu/996.ICU')\n",
    "# soup2 = make_soup('https://github.com/vuejs/vue')\n",
    "# soup3 = make_soup('https://github.com/EbookFoundation/free-programming-books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Language\n",
    "# soup.select('article', class_=\"markdown-body entry-content container-lg\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup1.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup2.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup3.select('li.d-inline:nth-child(1) > a:nth-child(1)')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def no_digit(s):\n",
    "#     no_digits = []\n",
    "#     # Iterate through the string, adding non-numbers to the no_digits list\n",
    "#     for i in s:\n",
    "#         if not i.isdigit():\n",
    "#             no_digits.append(i)\n",
    "#         return no_digits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
